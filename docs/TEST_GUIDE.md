# ğŸ§ª í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ (Test Guide)

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![pytest](https://img.shields.io/badge/pytest-7.0+-green.svg)](https://pytest.org)

> ğŸ”¬ Chat Keyword Batch Processorì˜ **ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ**

---

## ğŸ“‹ ëª©ì°¨

- [ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸](#-ë¹ ë¥¸-í…ŒìŠ¤íŠ¸)
- [ğŸ§ª í…ŒìŠ¤íŠ¸ ìœ í˜•](#-í…ŒìŠ¤íŠ¸-ìœ í˜•)
- [âš™ï¸ ì„¤ì • í…ŒìŠ¤íŠ¸](#ï¸-ì„¤ì •-í…ŒìŠ¤íŠ¸)
- [ğŸ”§ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸](#-ë‹¨ìœ„-í…ŒìŠ¤íŠ¸)
- [ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸](#-í†µí•©-í…ŒìŠ¤íŠ¸)
- [ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸](#-ì„±ëŠ¥-í…ŒìŠ¤íŠ¸)
- [ğŸ†• ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸](#-ëˆ„ë½-ë°ì´í„°-í…ŒìŠ¤íŠ¸)
- [ğŸ³ Docker í…ŒìŠ¤íŠ¸](#-docker-í…ŒìŠ¤íŠ¸)
- [ğŸ“ˆ ë¶€í•˜ í…ŒìŠ¤íŠ¸](#-ë¶€í•˜-í…ŒìŠ¤íŠ¸)
- [ğŸ› ï¸ í…ŒìŠ¤íŠ¸ ë„êµ¬](#ï¸-í…ŒìŠ¤íŠ¸-ë„êµ¬)
- [âŒ ë¬¸ì œ í•´ê²°](#-ë¬¸ì œ-í•´ê²°)

---

## ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

### 1ï¸âƒ£ **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸**

```bash
# ğŸ”§ ì„¤ì • ê²€ì¦
python main_batch.py --validate-config

# ğŸ§ª ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸
python test_missing_data.py

# ğŸ” ë¹ ë¥¸ ëˆ„ë½ ë°ì´í„° í™•ì¸
python run_missing_check.py 2025-06-11 2025-06-19

# ğŸ“Š ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
python main_report.py yesterday --output-dir test_reports
```

### 2ï¸âƒ£ **Docker í™˜ê²½ í…ŒìŠ¤íŠ¸**

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker-compose ps

# í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸
docker-compose exec keyword-batch /app/healthcheck.sh

# ì„¤ì • ê²€ì¦
docker-compose exec keyword-batch python main_batch.py --validate-config
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ìœ í˜•

### ğŸ“Š **í…ŒìŠ¤íŠ¸ ë¶„ë¥˜**

| í…ŒìŠ¤íŠ¸ ìœ í˜• | ëª©ì  | ì†Œìš” ì‹œê°„ | ì‹¤í–‰ ë¹ˆë„ |
|-------------|------|-----------|-----------|
| **ğŸ”§ ì„¤ì • í…ŒìŠ¤íŠ¸** | í™˜ê²½ ì„¤ì • ê²€ì¦ | < 1ë¶„ | ë§¤ì¼ |
| **ğŸ§ª ë‹¨ìœ„ í…ŒìŠ¤íŠ¸** | ê°œë³„ í•¨ìˆ˜ ê²€ì¦ | < 5ë¶„ | ì»¤ë°‹ë§ˆë‹¤ |
| **ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸** | ì„œë¹„ìŠ¤ ê°„ ì—°ë™ | 5-10ë¶„ | ë°°í¬ ì „ |
| **ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸** | ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ | 10-30ë¶„ | ì£¼ê°„ |
| **ğŸ“ˆ ë¶€í•˜ í…ŒìŠ¤íŠ¸** | ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ | 30ë¶„+ | ì›”ê°„ |

### ğŸ¯ **í…ŒìŠ¤íŠ¸ ì „ëµ**

```mermaid
graph TD
    A[ì½”ë“œ ë³€ê²½] --> B[ë‹¨ìœ„ í…ŒìŠ¤íŠ¸]
    B --> C[í†µí•© í…ŒìŠ¤íŠ¸]
    C --> D[ì„±ëŠ¥ í…ŒìŠ¤íŠ¸]
    D --> E[ë°°í¬]
    
    F[ì •ê¸° ì ê²€] --> G[ë¶€í•˜ í…ŒìŠ¤íŠ¸]
    G --> H[ë³´ì•ˆ í…ŒìŠ¤íŠ¸]
    H --> I[ìµœì í™”]
```

---

## âš™ï¸ ì„¤ì • í…ŒìŠ¤íŠ¸

### âœ… **ê¸°ë³¸ ì„¤ì • ê²€ì¦**

```bash
# ëª¨ë“  ì„¤ì • ê²€ì¦
python main_batch.py --validate-config

# ê°œë³„ ì„¤ì • ê²€ì¦
python -c "
from core.config import Config
config = Config()
print('âœ… ë°ì´í„°ë² ì´ìŠ¤:', config.database.validate())
print('âœ… HCX API:', config.hcx.validate())
print('âœ… ì´ë©”ì¼:', config.email.validate())
print('âœ… ë³´ê³ ì„œ:', config.report.validate())
"
```

### ğŸ” **ìƒì„¸ ì„¤ì • ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**

<details>
<summary><b>ğŸ”½ validate_config_detailed.py</b></summary>

```python
#!/usr/bin/env python3
"""ìƒì„¸ ì„¤ì • ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸"""

import asyncio
import sys
from typing import Dict, Any
from core.config import Config
from core.database import DatabaseManager
from services.hcx_service import HCXService
from services.email_service import EmailService

class ConfigValidator:
    def __init__(self):
        self.config = Config()
        self.results: Dict[str, Dict[str, Any]] = {}
    
    async def validate_all(self) -> bool:
        """ëª¨ë“  ì„¤ì •ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        print("ğŸ” ì„¤ì • ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
        
        # ê° êµ¬ì„± ìš”ì†Œ ê²€ì¦
        await self.validate_database()
        await self.validate_hcx_api()
        await self.validate_email()
        self.validate_directories()
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results()
        
        # ì „ì²´ ì„±ê³µ ì—¬ë¶€ ë°˜í™˜
        return all(
            result.get('success', False) 
            for category in self.results.values() 
            for result in category.values()
        )
    
    async def validate_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê²€ì¦"""
        print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì¤‘...")
        self.results['database'] = {}
        
        try:
            db_manager = DatabaseManager(self.config.database)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            connected = await db_manager.check_connection()
            self.results['database']['connection'] = {
                'success': connected,
                'message': 'ì—°ê²° ì„±ê³µ' if connected else 'ì—°ê²° ì‹¤íŒ¨'
            }
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            tables = ['admin_chat_keywords', 'admin_categories', 'chattings']
            for table in tables:
                try:
                    schema = await db_manager.get_table_schema(table)
                    self.results['database'][f'table_{table}'] = {
                        'success': len(schema) > 0,
                        'message': f'{len(schema)}ê°œ ì»¬ëŸ¼ í™•ì¸'
                    }
                except Exception as e:
                    self.results['database'][f'table_{table}'] = {
                        'success': False,
                        'message': f'í…Œì´ë¸” ì ‘ê·¼ ì‹¤íŒ¨: {e}'
                    }
                    
        except Exception as e:
            self.results['database']['connection'] = {
                'success': False,
                'message': f'ì—°ê²° ì˜¤ë¥˜: {e}'
            }
    
    async def validate_hcx_api(self):
        """HCX API ê²€ì¦"""
        print("ğŸ¤– HCX API ê²€ì¦ ì¤‘...")
        self.results['hcx'] = {}
        
        try:
            hcx_service = HCXService(self.config.hcx)
            
            # API í‚¤ ê²€ì¦
            result = hcx_service.classify_education_question("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            
            self.results['hcx']['api_key'] = {
                'success': result is not None,
                'message': 'API ì‘ë‹µ ì •ìƒ' if result else 'API ì‘ë‹µ ì—†ìŒ'
            }
            
            # ëª¨ë¸ ê²€ì¦
            if result:
                self.results['hcx']['model'] = {
                    'success': True,
                    'message': f'ëª¨ë¸ {self.config.hcx.model} ì •ìƒ ì‘ë™'
                }
            
        except Exception as e:
            self.results['hcx']['api_key'] = {
                'success': False,
                'message': f'API ì˜¤ë¥˜: {e}'
            }
    
    async def validate_email(self):
        """ì´ë©”ì¼ ì„¤ì • ê²€ì¦"""
        print("ğŸ“§ ì´ë©”ì¼ ì„¤ì • ê²€ì¦ ì¤‘...")
        self.results['email'] = {}
        
        try:
            email_service = EmailService(self.config.email)
            
            # SMTP ì—°ê²° í…ŒìŠ¤íŠ¸
            smtp_valid = email_service.validate_smtp_connection()
            self.results['email']['smtp'] = {
                'success': smtp_valid,
                'message': 'SMTP ì—°ê²° ì„±ê³µ' if smtp_valid else 'SMTP ì—°ê²° ì‹¤íŒ¨'
            }
            
            # ìˆ˜ì‹ ì ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
            recipients = self.config.email.recipient_emails
            valid_emails = [email for email in recipients if '@' in email]
            
            self.results['email']['recipients'] = {
                'success': len(valid_emails) == len(recipients),
                'message': f'{len(valid_emails)}/{len(recipients)}ê°œ ìœ íš¨í•œ ì´ë©”ì¼'
            }
            
        except Exception as e:
            self.results['email']['smtp'] = {
                'success': False,
                'message': f'ì´ë©”ì¼ ì˜¤ë¥˜: {e}'
            }
    
    def validate_directories(self):
        """ë””ë ‰í† ë¦¬ ê¶Œí•œ ê²€ì¦"""
        print("ğŸ“ ë””ë ‰í† ë¦¬ ê¶Œí•œ ê²€ì¦ ì¤‘...")
        self.results['directories'] = {}
        
        import os
        
        directories = [
            ('reports', self.config.report.output_dir),
            ('logs', 'logs'),
            ('temp', 'temp')
        ]
        
        for name, path in directories:
            try:
                # ë””ë ‰í† ë¦¬ ìƒì„± ì‹œë„
                os.makedirs(path, exist_ok=True)
                
                # ì“°ê¸° ê¶Œí•œ í…ŒìŠ¤íŠ¸
                test_file = os.path.join(path, 'test_write.tmp')
                with open(test_file, 'w') as f:
                    f.write('test')
                os.remove(test_file)
                
                self.results['directories'][name] = {
                    'success': True,
                    'message': f'{path} ì½ê¸°/ì“°ê¸° ê¶Œí•œ ì •ìƒ'
                }
                
            except Exception as e:
                self.results['directories'][name] = {
                    'success': False,
                    'message': f'{path} ê¶Œí•œ ì˜¤ë¥˜: {e}'
                }
    
    def print_results(self):
        """ê²€ì¦ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*60)
        print("ğŸ” ì„¤ì • ê²€ì¦ ê²°ê³¼")
        print("="*60)
        
        for category, tests in self.results.items():
            print(f"\nğŸ“‚ {category.upper()}")
            print("-" * 40)
            
            for test_name, result in tests.items():
                status = "âœ…" if result['success'] else "âŒ"
                print(f"  {status} {test_name:20}: {result['message']}")
        
        # ìš”ì•½
        total_tests = sum(len(tests) for tests in self.results.values())
        passed_tests = sum(
            1 for tests in self.results.values() 
            for result in tests.values() 
            if result['success']
        )
        
        print("\n" + "="*60)
        print(f"ğŸ“Š ìš”ì•½: {passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼")
        print("="*60)

async def main():
    validator = ConfigValidator()
    success = await validator.validate_all()
    
    if success:
        print("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì •ìƒì…ë‹ˆë‹¤!")
        sys.exit(0)
    else:
        print("âŒ ì¼ë¶€ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

</details>

---

## ğŸ”§ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### ğŸ§ª **pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸**

```bash
# pytest ì„¤ì¹˜
pip install pytest pytest-asyncio pytest-cov

# ëª¨ë“  ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/unit/ -v

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ ì‹¤í–‰
pytest tests/unit/ --cov=services --cov=core --cov-report=html
```

### ğŸ“ **í…ŒìŠ¤íŠ¸ íŒŒì¼ êµ¬ì¡°**

```
tests/
â”œâ”€â”€ unit/                           # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_config.py             # ì„¤ì • í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_database.py           # ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_hcx_service.py        # HCX ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_email_service.py      # ì´ë©”ì¼ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_batch_service.py      # ë°°ì¹˜ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ integration/                    # í†µí•© í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_end_to_end.py         # ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_missing_data.py       # ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸
â”œâ”€â”€ performance/                    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_batch_performance.py  # ë°°ì¹˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_memory_usage.py       # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
â””â”€â”€ fixtures/                      # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    â”œâ”€â”€ sample_data.json
    â””â”€â”€ test_config.env
```

### ğŸ¯ **í•µì‹¬ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ**

<details>
<summary><b>ğŸ”½ test_batch_service.py</b></summary>

```python
import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from services.batch_service import BatchService
from core.config import Config

class TestBatchService:
    
    @pytest.fixture
    def config(self):
        """í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ìƒì„±"""
        return Config()
    
    @pytest.fixture
    def batch_service(self, config):
        """í…ŒìŠ¤íŠ¸ìš© ë°°ì¹˜ ì„œë¹„ìŠ¤ ìƒì„±"""
        return BatchService(config)
    
    @pytest.mark.asyncio
    async def test_run_single_batch_success(self, batch_service):
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        with patch.object(batch_service, '_fetch_data_for_period') as mock_fetch:
            mock_fetch.return_value = [
                ('í…ŒìŠ¤íŠ¸ ì§ˆë¬¸', 5, '2025-06-19'),
                ('ìˆ˜ê°•ì‹ ì²­ ì§ˆë¬¸', 3, '2025-06-19')
            ]
            
            with patch.object(batch_service, '_process_batch_data_parallel') as mock_process:
                mock_process.return_value = (10, 2)  # processed, skipped
                
                result = await batch_service.run_single_batch('2025-06-19')
                
                assert result['status'] == 'SUCCESS'
                assert result['processed_count'] == 10
                assert result['skipped_count'] == 2
                assert result['total_rows'] == 2
    
    @pytest.mark.asyncio
    async def test_check_missing_data(self, batch_service):
        """ëˆ„ë½ ë°ì´í„° í™•ì¸ í…ŒìŠ¤íŠ¸"""
        with patch.object(batch_service.db_manager, 'call_procedure') as mock_proc:
            with patch.object(batch_service.db_manager, 'execute_query') as mock_query:
                # í”„ë¡œì‹œì € í˜¸ì¶œ ëª¨í‚¹
                mock_proc.return_value = True
                
                # ì¿¼ë¦¬ ê²°ê³¼ ëª¨í‚¹
                mock_query.side_effect = [
                    [('2025-06-19', 100)],  # ì²˜ë¦¬ëœ ë°ì´í„°
                    [('2025-06-19', 10)]    # ëˆ„ë½ëœ ë°ì´í„°
                ]
                
                result = await batch_service.check_missing_data('2025-06-19', '2025-06-19')
                
                assert result['status'] == 'SUCCESS'
                assert result['total_processed'] == 100
                assert result['total_missing'] == 10
    
    @pytest.mark.asyncio
    async def test_process_missing_data_empty(self, batch_service):
        """ëˆ„ë½ ë°ì´í„° ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸"""
        with patch.object(batch_service, 'check_missing_data') as mock_check:
            mock_check.return_value = {
                'status': 'SUCCESS',
                'total_missing': 0,
                'total_processed': 100
            }
            
            result = await batch_service.process_missing_data('2025-06-19', '2025-06-19')
            
            assert result['status'] == 'SUCCESS'
            assert result['message'] == 'ëˆ„ë½ ë°ì´í„° ì—†ìŒ'
            assert result['processed_count'] == 0
    
    def test_format_duration(self, batch_service):
        """ì‹œê°„ í¬ë§·íŒ… í…ŒìŠ¤íŠ¸"""
        assert batch_service._format_duration(65) == "1ë¶„ 5ì´ˆ"
        assert batch_service._format_duration(30) == "0ë¶„ 30ì´ˆ"
        assert batch_service._format_duration(3661) == "61ë¶„ 1ì´ˆ"
```

</details>

---

## ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸

### ğŸŒ **ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸**

```python
# test_end_to_end.py
import pytest
import asyncio
from datetime import datetime, timedelta

@pytest.mark.asyncio
@pytest.mark.integration
async def test_complete_batch_workflow():
    """ì™„ì „í•œ ë°°ì¹˜ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    from core.config import Config
    from services.batch_service import BatchService
    
    config = Config()
    batch_service = BatchService(config)
    
    # í…ŒìŠ¤íŠ¸ ë‚ ì§œ (ì–´ì œ)
    test_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    # 1. ì„¤ì • ê²€ì¦
    assert config.validate_all(), "ì„¤ì • ê²€ì¦ ì‹¤íŒ¨"
    
    # 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
    db_connected = await batch_service.db_manager.check_connection()
    assert db_connected, "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
    
    # 3. HCX API ì—°ê²° í™•ì¸
    try:
        test_result = batch_service.hcx_service.classify_education_question("í…ŒìŠ¤íŠ¸")
        assert test_result is not None, "HCX API ì‘ë‹µ ì—†ìŒ"
    except Exception as e:
        pytest.skip(f"HCX API ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # 4. ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    result = await batch_service.run_single_batch(test_date)
    assert result['status'] == 'SUCCESS', f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {result}"
    
    # 5. ê²°ê³¼ ê²€ì¦
    assert result['total_rows'] >= 0, "ì´ í–‰ ìˆ˜ ìŒìˆ˜"
    assert result['processed_count'] >= 0, "ì²˜ë¦¬ ìˆ˜ ìŒìˆ˜"
    assert result['skipped_count'] >= 0, "ìŠ¤í‚µ ìˆ˜ ìŒìˆ˜"
    
    print(f"âœ… ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {result}")
```

### ğŸ”„ **ì„œë¹„ìŠ¤ ê°„ ì—°ë™ í…ŒìŠ¤íŠ¸**

```bash
# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/integration/ -v -m integration

# ì™¸ë¶€ ì˜ì¡´ì„± í¬í•¨ í…ŒìŠ¤íŠ¸
pytest tests/integration/ -v -m "integration and external"
```

---

## ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### âš¡ **ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥**

<details>
<summary><b>ğŸ”½ test_batch_performance.py</b></summary>

```python
import pytest
import time
import asyncio
from typing import List, Tuple
from services.batch_service import BatchService
from core.config import Config

class TestBatchPerformance:
    
    @pytest.fixture
    def performance_config(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì„¤ì •"""
        config = Config()
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        config.batch.batch_size = 20
        config.batch.parallel_workers = 4
        return config
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_batch_processing_speed(self, performance_config):
        """ë°°ì¹˜ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸"""
        batch_service = BatchService(performance_config)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (100ê°œ)
        test_data = [
            (f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i}", 1, "2025-06-19")
            for i in range(100)
        ]
        
        start_time = time.time()
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        processed, skipped = await batch_service._process_batch_data_parallel(
            test_data, 0, {'category_distribution': {}}, "performance_test"
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        items_per_second = len(test_data) / duration
        assert items_per_second > 10, f"ì²˜ë¦¬ ì†ë„ ë„ˆë¬´ ëŠë¦¼: {items_per_second:.2f} items/sec"
        
        print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼: {items_per_second:.2f} items/sec, ì´ {duration:.2f}ì´ˆ")
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_memory_usage(self, performance_config):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        batch_service = BatchService(performance_config)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„± (1000ê°œ)
        test_data = [
            (f"ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i}" * 10, 1, "2025-06-19")
            for i in range(1000)
        ]
        
        await batch_service._process_batch_data_parallel(
            test_data, 0, {'category_distribution': {}}, "memory_test"
        )
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ê²€ì¦ (100MB ì´í•˜)
        assert memory_increase < 100, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤: {memory_increase:.2f}MB ì¦ê°€"
        
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_increase:.2f}MB ì¦ê°€")
    
    @pytest.mark.asyncio
    @pytest.mark.benchmark
    async def test_hcx_api_response_time(self, performance_config):
        """HCX API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        from services.hcx_service import HCXService
        
        hcx_service = HCXService(performance_config.hcx)
        
        # ì—¬ëŸ¬ ìš”ì²­ì˜ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        response_times = []
        test_questions = [
            "ìˆ˜ê°•ì‹ ì²­ì€ ì–¸ì œ í•˜ë‚˜ìš”?",
            "ì„±ì  í™•ì¸ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "íœ´í•™ ì‹ ì²­ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ì¥í•™ê¸ˆ ì‹ ì²­ ê¸°ê°„ì€ ì–¸ì œì¸ê°€ìš”?",
            "ì „í•™ ì ˆì°¨ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤"
        ]
        
        for question in test_questions:
            start_time = time.time()
            result = hcx_service.classify_education_question(question)
            end_time = time.time()
            
            response_time = end_time - start_time
            response_times.append(response_time)
            
            assert result is not None, f"API ì‘ë‹µ ì—†ìŒ: {question}"
        
        avg_response_time = sum(response_times) / len(response_times)
        max_response_time = max(response_times)
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ì¤€ ê²€ì¦
        assert avg_response_time < 5.0, f"í‰ê·  ì‘ë‹µ ì‹œê°„ ê³¼ë‹¤: {avg_response_time:.2f}ì´ˆ"
        assert max_response_time < 10.0, f"ìµœëŒ€ ì‘ë‹µ ì‹œê°„ ê³¼ë‹¤: {max_response_time:.2f}ì´ˆ"
        
        print(f"ğŸ¤– HCX API ì„±ëŠ¥: í‰ê·  {avg_response_time:.2f}ì´ˆ, ìµœëŒ€ {max_response_time:.2f}ì´ˆ")
```

</details>

### ğŸ“ˆ **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰**

```bash
# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/performance/ -v -m performance

# ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/performance/ -v -m benchmark --benchmark-only

# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python -m memory_profiler test_batch_performance.py
```

---

## ğŸ†• ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸

### ğŸ” **ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸**

ê¸°ì¡´ì— ìˆëŠ” `test_missing_data.py` íŒŒì¼ì„ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_missing_data.py

# ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ ì‹¤í–‰
LOG_LEVEL=DEBUG python test_missing_data.py
```

### ğŸ“Š **ëˆ„ë½ ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸**

```python
# test_missing_data_scenarios.py
import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_missing_data_detection():
    """ëˆ„ë½ ë°ì´í„° íƒì§€ í…ŒìŠ¤íŠ¸"""
    from services.batch_service import BatchService
    from core.config import Config
    
    batch_service = BatchService(Config())
    
    # ëª¨í‚¹ëœ í”„ë¡œì‹œì € ê²°ê³¼
    with patch.object(batch_service.db_manager, 'call_procedure') as mock_proc, \
         patch.object(batch_service.db_manager, 'execute_query') as mock_query:
        
        mock_proc.return_value = True
        mock_query.side_effect = [
            [('2025-06-19', 50)],   # ì²˜ë¦¬ëœ ë°ì´í„°
            [('2025-06-19', 5)]     # ëˆ„ë½ëœ ë°ì´í„°
        ]
        
        result = await batch_service.check_missing_data('2025-06-19', '2025-06-19')
        
        assert result['status'] == 'SUCCESS'
        assert result['total_processed'] == 50
        assert result['total_missing'] == 5

@pytest.mark.asyncio
async def test_missing_data_processing():
    """ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    from services.batch_service import BatchService
    from core.config import Config
    
    batch_service = BatchService(Config())
    
    # ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ëª¨í‚¹
    with patch.object(batch_service, 'check_missing_data') as mock_check, \
         patch.object(batch_service.db_manager, 'execute_query') as mock_query, \
         patch.object(batch_service, '_process_batch_data_parallel') as mock_process:
        
        # ëˆ„ë½ ë°ì´í„° í™•ì¸ ê²°ê³¼
        mock_check.return_value = {
            'status': 'SUCCESS',
            'total_missing': 5,
            'total_processed': 50
        }
        
        # ëˆ„ë½ëœ ë°ì´í„° ì¡°íšŒ ê²°ê³¼
        mock_query.return_value = [
            ('ëˆ„ë½ëœ ì§ˆë¬¸ 1', 2, '2025-06-19'),
            ('ëˆ„ë½ëœ ì§ˆë¬¸ 2', 1, '2025-06-19')
        ]
        
        # ì²˜ë¦¬ ê²°ê³¼
        mock_process.return_value = (5, 0)  # processed, skipped
        
        result = await batch_service.process_missing_data('2025-06-19', '2025-06-19')
        
        assert result['status'] == 'SUCCESS'
        assert result['processed_count'] == 5
        assert result['skipped_count'] == 0
```

---

## ğŸ³ Docker í…ŒìŠ¤íŠ¸

### ğŸ“¦ **ì»¨í…Œì´ë„ˆ í…ŒìŠ¤íŠ¸**

```bash
# Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker-compose ps

# í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸
docker-compose exec keyword-batch /app/healthcheck.sh

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker-compose exec keyword-batch python -m pytest tests/unit/ -v
```

### ğŸ”§ **Docker í™˜ê²½ ê²€ì¦**

```python
# test_docker_environment.py
import subprocess
import pytest

def test_docker_container_running():
    """Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìƒíƒœ í™•ì¸"""
    result = subprocess.run(
        ['docker-compose', 'ps', '-q', 'keyword-batch'],
        capture_output=True, text=True
    )
    assert result.returncode == 0
    assert len(result.stdout.strip()) > 0, "ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"

def test_docker_environment_variables():
    """Docker í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    result = subprocess.run(
        ['docker-compose', 'exec', '-T', 'keyword-batch', 'env'],
        capture_output=True, text=True
    )
    assert result.returncode == 0
    
    env_vars = result.stdout
    required_vars = ['ENGINE_FOR_SQLALCHEMY', 'HCX_CHAT_API_KEY']
    
    for var in required_vars:
        assert var in env_vars, f"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ {var} ëˆ„ë½"

def test_docker_volumes():
    """Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸"""
    result = subprocess.run(
        ['docker-compose', 'exec', '-T', 'keyword-batch', 'ls', '-la', '/app/reports'],
        capture_output=True, text=True
    )
    assert result.returncode == 0, "reports ë””ë ‰í† ë¦¬ ì ‘ê·¼ ë¶ˆê°€"
```

---

## ğŸ“ˆ ë¶€í•˜ í…ŒìŠ¤íŠ¸

### ğŸš€ **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸**

```python
# test_load_testing.py
import pytest
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

@pytest.mark.asyncio
@pytest.mark.load
async def test_concurrent_batch_processing():
    """ë™ì‹œ ë°°ì¹˜ ì²˜ë¦¬ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    from services.batch_service import BatchService
    from core.config import Config
    
    config = Config()
    
    async def run_batch(date_suffix):
        """ê°œë³„ ë°°ì¹˜ ì‹¤í–‰"""
        batch_service = BatchService(config)
        test_date = f"2025-06-{date_suffix:02d}"
        
        try:
            result = await batch_service.run_single_batch(test_date)
            return result['status'] == 'SUCCESS'
        except Exception as e:
            print(f"ë°°ì¹˜ ì‹¤íŒ¨ ({test_date}): {e}")
            return False
    
    # ë™ì‹œì— ì—¬ëŸ¬ ë‚ ì§œ ì²˜ë¦¬
    dates = range(10, 20)  # 2025-06-10 ~ 2025-06-19
    
    start_time = time.time()
    
    # ë³‘ë ¬ ì‹¤í–‰
    tasks = [run_batch(date) for date in dates]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    end_time = time.time()
    duration = end_time - start_time
    
    # ê²°ê³¼ ê²€ì¦
    success_count = sum(1 for r in results if r is True)
    success_rate = success_count / len(results)
    
    print(f"ğŸ“Š ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {duration:.2f}ì´ˆ")
    print(f"   - ì„±ê³µë¥ : {success_rate:.2%} ({success_count}/{len(results)})")
    print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {duration/len(results):.2f}ì´ˆ/ë°°ì¹˜")
    
    # ì„±ê³µë¥  80% ì´ìƒ ìš”êµ¬
    assert success_rate >= 0.8, f"ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì„±ê³µë¥  {success_rate:.2%}"

@pytest.mark.load
def test_memory_stability():
    """ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # ë°˜ë³µ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸
    for i in range(100):
        # ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ë° ì²˜ë¦¬
        large_data = [f"í…ŒìŠ¤íŠ¸ ë°ì´í„° {j}" * 100 for j in range(1000)]
        
        # ë°ì´í„° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        processed = [data.upper() for data in large_data]
        del large_data, processed
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì²´í¬
        if i % 20 == 0:
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_increase = current_memory - initial_memory
            
            print(f"ë°˜ë³µ {i}: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ {current_memory:.2f}MB (+{memory_increase:.2f}MB)")
            
            # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ì œí•œ (500MB)
            assert memory_increase < 500, f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: {memory_increase:.2f}MB ì¦ê°€"
```

### ğŸ“Š **ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰**

```bash
# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì˜: ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
pytest tests/performance/ -v -m load --timeout=1800

# íŠ¹ì • ë¶€í•˜ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/performance/test_load_testing.py::test_concurrent_batch_processing -v
```

---

## ğŸ› ï¸ í…ŒìŠ¤íŠ¸ ë„êµ¬

### ğŸ“‹ **í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**

<details>
<summary><b>ğŸ”½ run_all_tests.sh</b></summary>

```bash
#!/bin/bash

echo "ğŸ§ª ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"
echo "=========================="

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
export TESTING=true
export LOG_LEVEL=WARNING

# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p test_results

echo "1ï¸âƒ£ ì„¤ì • ê²€ì¦ í…ŒìŠ¤íŠ¸..."
python main_batch.py --validate-config > test_results/config_validation.log 2>&1
if [ $? -eq 0 ]; then
    echo "   âœ… ì„¤ì • ê²€ì¦ í†µê³¼"
else
    echo "   âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨"
    exit 1
fi

echo "2ï¸âƒ£ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸..."
pytest tests/unit/ -v --cov=services --cov=core \
    --cov-report=html:test_results/coverage_html \
    --cov-report=term \
    --junit-xml=test_results/unit_tests.xml > test_results/unit_tests.log 2>&1

if [ $? -eq 0 ]; then
    echo "   âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼"
else
    echo "   âŒ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
    cat test_results/unit_tests.log
    exit 1
fi

echo "3ï¸âƒ£ í†µí•© í…ŒìŠ¤íŠ¸..."
pytest tests/integration/ -v -m integration \
    --junit-xml=test_results/integration_tests.xml > test_results/integration_tests.log 2>&1

if [ $? -eq 0 ]; then
    echo "   âœ… í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼"
else
    echo "   âš ï¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì™¸ë¶€ ì˜ì¡´ì„± ë¬¸ì œì¼ ìˆ˜ ìˆìŒ)"
fi

echo "4ï¸âƒ£ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸..."
pytest tests/performance/ -v -m performance \
    --junit-xml=test_results/performance_tests.xml > test_results/performance_tests.log 2>&1

if [ $? -eq 0 ]; then
    echo "   âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼"
else
    echo "   âš ï¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
fi

echo "5ï¸âƒ£ ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸..."
python test_missing_data.py > test_results/missing_data_test.log 2>&1

if [ $? -eq 0 ]; then
    echo "   âœ… ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸ í†µê³¼"
else
    echo "   âš ï¸ ëˆ„ë½ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
fi

echo ""
echo "ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:"
echo "   - ì„¤ì • ê²€ì¦: âœ…"
echo "   - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: âœ…"
echo "   - í†µí•© í…ŒìŠ¤íŠ¸: âš ï¸"
echo "   - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: âš ï¸"
echo "   - ëˆ„ë½ ë°ì´í„°: âš ï¸"
echo ""
echo "ğŸ“ ìƒì„¸ ê²°ê³¼: test_results/ ë””ë ‰í† ë¦¬ í™•ì¸"
echo "ğŸŒ ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸: test_results/coverage_html/index.html"
```

</details>

### ğŸ”§ **pytest ì„¤ì • íŒŒì¼**

```ini
# pytest.ini
[tool:pytest]
minversion = 7.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --disable-warnings
testpaths = tests
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    load: Load tests
    benchmark: Benchmark tests
    external: Tests requiring external dependencies
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
```

---

## âŒ ë¬¸ì œ í•´ê²°

### ğŸ”§ **ì¼ë°˜ì ì¸ í…ŒìŠ¤íŠ¸ ë¬¸ì œ**

#### 1. **í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì˜¤ë¥˜**
```bash
# ë¬¸ì œ: ModuleNotFoundError
# í•´ê²°ë°©ë²•:
export PYTHONPATH=$PYTHONPATH:$(pwd)
pip install -e .
```

#### 2. **ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨**
```bash
# ë¬¸ì œ: RuntimeError: no running event loop
# í•´ê²°ë°©ë²•:
pip install pytest-asyncio
# pytest.iniì— asyncio_mode = auto ì¶”ê°€
```

#### 3. **ì™¸ë¶€ ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨**
```bash
# ë¬¸ì œ: HCX API ì—°ê²° ì‹¤íŒ¨
# í•´ê²°ë°©ë²•:
pytest tests/ -m "not external"  # ì™¸ë¶€ ì˜ì¡´ì„± ì œì™¸
```

### ğŸ” **í…ŒìŠ¤íŠ¸ ë””ë²„ê¹…**

```bash
# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
pytest tests/ -v -s --log-cli-level=DEBUG

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/unit/test_batch_service.py::TestBatchService::test_run_single_batch_success -v

# ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë§Œ ì¬ì‹¤í–‰
pytest --lf -v

# íŠ¹ì • ë§ˆì»¤ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest -m "unit and not external" -v
```

### ğŸ“ **ì¶”ê°€ ì§€ì›**

í…ŒìŠ¤íŠ¸ ê´€ë ¨ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´:

1. **ë¡œê·¸ í™•ì¸**: `test_results/` ë””ë ‰í† ë¦¬ì˜ ìƒì„¸ ë¡œê·¸
2. **ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸**: `test_results/coverage_html/index.html`
3. **ë¬¸ì˜í•˜ê¸°**: [ksy9744@clabi.co.kr](mailto:ksy9744@clabi.co.kr)

---

<div align="center">

**ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025ë…„ 6ì›” 19ì¼  
**ğŸ§ª í…ŒìŠ¤íŠ¸ ë²„ì „**: v1.0  
**ğŸ’¡ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´**: [ksy9744@clabi.co.kr](mailto:ksy9744@clabi.co.kr)

---

*ì² ì €í•œ í…ŒìŠ¤íŠ¸ë¡œ ì•ˆì •ì ì¸ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ê°€ìš”! ğŸš€*

</div> 