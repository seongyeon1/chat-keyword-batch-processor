"""
ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ëª¨ë“ˆ - ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§ê³¼ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import asyncio
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, List, Tuple
from concurrent.futures import ThreadPoolExecutor
import time

from core.config import Config, BatchConfig
from core.database import DatabaseManager
from core.exceptions import BatchProcessError, DatabaseError
from services.hcx_service import HCXService
from services.email_service import EmailService
from services.excel_service import ExcelService
from utils.date_utils import DateUtils
from utils.logger import setup_logging, get_logger, log_info, log_warning, log_error, log_debug
from queries.batch_queries import BatchQueries


class BatchService:
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        self.config = config
        
        # ğŸš€ ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        setup_logging(config.log)
        self.logger = get_logger("batch_service")
        
        self.db_manager = DatabaseManager(config.database)
        self.hcx_service = HCXService(config.hcx)
        self.email_service = EmailService(config.email)
        self.excel_service = ExcelService(config.report, self.db_manager)
        self.batch_created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        self.queries = BatchQueries(config)
        
        log_info("âœ… BatchService ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def run_batch_range(self, start_date: str, end_date: str, start_index: int = 0) -> Dict[str, Any]:
        """ê¸°ê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. (ë‚ ì§œë³„ ë³‘ë ¬ì²˜ë¦¬)"""
        log_info(f"ğŸ“… ê¸°ê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {start_date} ~ {end_date}")
        
        result = {
            "status": "SUCCESS",
            "start_date": start_date,
            "end_date": end_date,
            "total_rows": 0,
            "processed_count": 0,
            "skipped_count": 0,
            "duration": None,
            "details": []
        }
        
        try:
            start_time = time.time()
            
            # ë‚ ì§œ ë²”ìœ„ ìƒì„±
            date_range = DateUtils.generate_date_range(start_date, end_date)
            log_info(f"ğŸ“‹ ì²˜ë¦¬í•  ë‚ ì§œ: {len(date_range)}ì¼")
            
            # ğŸš€ ë‚ ì§œë³„ ë³‘ë ¬ì²˜ë¦¬ êµ¬í˜„
            if len(date_range) > 1:
                log_info(f"ğŸš€ ë‚ ì§œë³„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(date_range)}ê°œ ë‚ ì§œ")
                
                # ë³‘ë ¬ ì²˜ë¦¬ - ë™ì‹œì— ì—¬ëŸ¬ ë‚ ì§œ ì²˜ë¦¬
                date_tasks = [
                    self.run_single_batch(target_date, 0)
                    for target_date in date_range
                ]
                
                # ë³‘ë ¬ ì‹¤í–‰
                date_results = await asyncio.gather(*date_tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for i, target_date in enumerate(date_range):
                    date_result = date_results[i]
                    
                    if isinstance(date_result, Exception):
                        log_error(f"âŒ {target_date} ì²˜ë¦¬ ì‹¤íŒ¨: {date_result}")
                        result["details"].append({
                            "date": target_date,
                            "status": "FAILED",
                            "processed": 0,
                            "skipped": 0,
                            "error": str(date_result)
                        })
                        continue
                    
                    # ì„±ê³µí•œ ê²½ìš° ê²°ê³¼ ëˆ„ì 
                    result["total_rows"] += date_result.get("total_rows", 0)
                    result["processed_count"] += date_result.get("processed_count", 0)
                    result["skipped_count"] += date_result.get("skipped_count", 0)
                    result["details"].append({
                        "date": target_date,
                        "status": date_result.get("status", "UNKNOWN"),
                        "processed": date_result.get("processed_count", 0),
                        "skipped": date_result.get("skipped_count", 0)
                    })
                    
                    log_info(f"âœ… {target_date} ì™„ë£Œ: {date_result.get('processed_count', 0)}ê°œ ì²˜ë¦¬")
                
                log_info(f"ğŸ‰ ë‚ ì§œë³„ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ!")
                
            else:
                # ë‹¨ì¼ ë‚ ì§œì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹
                target_date = date_range[0]
                log_info(f"ğŸ“… ë‹¨ì¼ ë‚ ì§œ ì²˜ë¦¬: {target_date}")
                
                date_result = await self.run_single_batch(target_date, 0)
                
                result["total_rows"] += date_result.get("total_rows", 0)
                result["processed_count"] += date_result.get("processed_count", 0)
                result["skipped_count"] += date_result.get("skipped_count", 0)
                result["details"].append({
                    "date": target_date,
                    "status": date_result.get("status", "UNKNOWN"),
                    "processed": date_result.get("processed_count", 0),
                    "skipped": date_result.get("skipped_count", 0)
                })
                
                log_info(f"âœ… {target_date} ì™„ë£Œ: {date_result.get('processed_count', 0)}ê°œ ì²˜ë¦¬")
            
            end_time = time.time()
            duration = self._format_duration(end_time - start_time)
            result["duration"] = duration
            
            # ì‹¤íŒ¨í•œ ë‚ ì§œê°€ ìˆëŠ”ì§€ í™•ì¸
            failed_dates = [detail["date"] for detail in result["details"] if detail["status"] == "FAILED"]
            if failed_dates:
                result["status"] = "PARTIAL_SUCCESS"
                result["failed_dates"] = failed_dates
                log_warning(f"âš ï¸ ì¼ë¶€ ë‚ ì§œ ì²˜ë¦¬ ì‹¤íŒ¨: {', '.join(failed_dates)}")
            
            log_info(f"ğŸ‰ ê¸°ê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
            return result
            
        except Exception as e:
            result["status"] = "FAILED"
            result["error"] = str(e)
            log_error(f"âŒ ê¸°ê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise BatchProcessError(f"ê¸°ê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    async def check_missing_data(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """ëˆ„ë½ëœ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        log_info(f"ğŸ” ëˆ„ë½ ë°ì´í„° í™•ì¸ ì¤‘: {start_date} ~ {end_date}")
        
        try:
            # 1. ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ì¡°íšŒ (classify_chat_keywords_by_date ì¿¼ë¦¬ ëŒ€ì²´)
            log_info("ğŸ“Š ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            classified_query = self.queries.classify_chat_keywords_by_date(start_date, end_date)
            classified_result = await self.db_manager.execute_query(classified_query)
            
            # 2. í•´ë‹¹ ê¸°ê°„ì˜ ì „ì²´ ì±„íŒ… ë°ì´í„° ì¡°íšŒ
            log_info("ğŸ“Š ì „ì²´ ì±„íŒ… ë°ì´í„° ì¡°íšŒ ì¤‘...")
            total_chattings_query = self.queries.get_total_chattings_by_date(start_date, end_date)
            total_result = await self.db_manager.execute_query(total_chattings_query)
            
            # 3. ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ë¶„ì„
            log_info("ğŸ“Š ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ë¶„ì„ ì¤‘...")
            
            # ì²˜ë¦¬ëœ ì§ˆë¬¸ë“¤ ì¶”ì¶œ
            classified_questions = set()
            classified_by_date = {}
            
            for row in classified_result:
                # ì¿¼ë¦¬ ê²°ê³¼: category_name, query_text, keyword, created_at, query_count
                query_text = row[1]
                created_at = row[3]
                date_str = created_at.strftime('%Y-%m-%d') if hasattr(created_at, 'strftime') else str(created_at)[:10]
                
                classified_questions.add(query_text)
                
                if date_str not in classified_by_date:
                    classified_by_date[date_str] = set()
                classified_by_date[date_str].add(query_text)
            
            # 4. ì „ì²´ ì±„íŒ…ì—ì„œ ê³ ìœ  ì§ˆë¬¸ë“¤ ì¡°íšŒ
            log_info("ğŸ” ì „ì²´ ê³ ìœ  ì§ˆë¬¸ ì¡°íšŒ ì¤‘...")
            all_questions_query = self.queries.get_all_unique_questions_by_date(start_date, end_date)
            all_questions_result = await self.db_manager.execute_query(all_questions_query)
            
            # 5. ëˆ„ë½ ë°ì´í„° ë¶„ì„
            all_questions_by_date = {}
            missing_questions_by_date = {}
            
            for row in all_questions_result:
                input_text = row[0]
                date_obj = row[1]
                date_str = date_obj.strftime('%Y-%m-%d') if hasattr(date_obj, 'strftime') else str(date_obj)
                
                if date_str not in all_questions_by_date:
                    all_questions_by_date[date_str] = set()
                all_questions_by_date[date_str].add(input_text)
                
                # ëˆ„ë½ëœ ì§ˆë¬¸ í™•ì¸
                if input_text not in classified_questions:
                    if date_str not in missing_questions_by_date:
                        missing_questions_by_date[date_str] = set()
                    missing_questions_by_date[date_str].add(input_text)
            
            # 6. ê²°ê³¼ ì •ë¦¬
            total_summary = {row[0].strftime('%Y-%m-%d'): {
                'unique_questions': row[1], 
                'total_messages': row[2]
            } for row in total_result}
            
            processed_summary = {}
            for date_str, questions in classified_by_date.items():
                processed_summary[date_str] = {
                    'processed_questions': len(questions)
                }
            
            missing_summary = {}
            for date_str, questions in missing_questions_by_date.items():
                missing_summary[date_str] = {
                    'missing_questions': len(questions)
                }
            
            # 7. í†µê³„ ê³„ì‚°
            total_unique_questions = sum(day['unique_questions'] for day in total_summary.values())
            total_processed_questions = len(classified_questions)
            total_missing_questions = sum(len(questions) for questions in missing_questions_by_date.values())
            
            log_info(f"âœ… ëˆ„ë½ ë°ì´í„° í™•ì¸ ì™„ë£Œ")
            log_info(f"ğŸ“Š ì „ì²´ ê³ ìœ  ì§ˆë¬¸: {total_unique_questions}ê°œ")
            log_info(f"âœ… ê¸°ì¡´ ì²˜ë¦¬ëœ ì§ˆë¬¸: {total_processed_questions}ê°œ")
            log_info(f"ğŸš« ëˆ„ë½ëœ ì§ˆë¬¸: {total_missing_questions}ê°œ")
            log_info(f"ğŸ“ˆ ì²˜ë¦¬ìœ¨: {(total_processed_questions / total_unique_questions * 100):.1f}%" if total_unique_questions > 0 else "ì²˜ë¦¬ìœ¨: 0%")
            
            return {
                "status": "SUCCESS",
                "period": f"{start_date} ~ {end_date}",
                "total_summary": total_summary,
                "processed_summary": processed_summary,
                "missing_summary": missing_summary,
                "stats": {
                    "total_unique_questions": total_unique_questions,
                    "total_processed_questions": total_processed_questions,
                    "total_missing_questions": total_missing_questions,
                    "processing_rate": round(total_processed_questions / total_unique_questions * 100, 1) if total_unique_questions > 0 else 0
                },
                "classified_result_count": len(classified_result),
                "missing_questions_detail": missing_questions_by_date
            }
            
        except Exception as e:
            raise BatchProcessError(f"ëˆ„ë½ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")

    async def process_missing_data(self, start_date: str, end_date: str, start_index: int = 0, limit: int = None) -> Dict[str, Any]:
        """ëˆ„ë½ëœ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        limit_text = f" (ìµœëŒ€ {limit}ê°œ ì œí•œ)" if limit else ""
        log_info(f"ğŸ”§ ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {start_date} ~ {end_date}{limit_text}")
        
        try:
            # 1. ì •í™•í•œ ëˆ„ë½ ë°ì´í„° ì¡°íšŒ
            log_info("ğŸ” ì •í™•í•œ ëˆ„ë½ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            
            missing_data_query = self.queries.get_missing_data(start_date, end_date)
            missing_rows = await self.db_manager.execute_query(missing_data_query)
            
            if not missing_rows:
                log_info("âœ… ëˆ„ë½ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "SUCCESS",
                    "message": "ëˆ„ë½ ë°ì´í„° ì—†ìŒ",
                    "processed_count": 0,
                    "skipped_count": 0,
                    "period": f"{start_date} ~ {end_date}",
                    "limit_applied": limit
                }
            
            actual_count = len(missing_rows)
            limit_applied_text = f" (ì œí•œ: {limit}ê°œ ì ìš©)" if limit and actual_count == limit else ""
            log_info(f"ğŸ“‹ ë°œê²¬ëœ ëˆ„ë½ ë°ì´í„°: {actual_count}ê°œ{limit_applied_text}")
            
            if limit and actual_count < limit:
                log_info(f"â„¹ï¸ ì‹¤ì œ ëˆ„ë½ ë°ì´í„°ê°€ ì œí•œê°’({limit}ê°œ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤.")
            
            # ë‚ ì§œë³„ ëˆ„ë½ ë°ì´í„° ìš”ì•½ ì¶œë ¥
            missing_by_date = {}
            for row in missing_rows:
                missing_date = str(row[0])
                if missing_date not in missing_by_date:
                    missing_by_date[missing_date] = 0
                missing_by_date[missing_date] += 1
            
            log_info("ğŸ“… ë‚ ì§œë³„ ëˆ„ë½ ë°ì´í„° ìš”ì•½:")
            for date, count in sorted(missing_by_date.items()):
                log_info(f"   - {date}: {count}ê°œ")
            
            # 2. ëˆ„ë½ëœ ë°ì´í„°ë¥¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
            start_time = time.time()
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° í˜•ì‹ ë³€í™˜
            batch_rows = []
            for row in missing_rows:
                missing_date = row[0]
                input_text = row[1]
                missing_count = row[2]
                
                # created_atì„ ë§Œë“¤ê¸° ìœ„í•´ missing_date ì‚¬ìš©
                try:
                    from datetime import datetime
                    if isinstance(missing_date, str):
                        created_at = datetime.strptime(missing_date, '%Y-%m-%d')
                    else:
                        created_at = missing_date
                except:
                    created_at = missing_date
                
                batch_rows.append((input_text, missing_count, created_at))
            
            # stats ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
            stats = {
                'target': f"missing_data_{start_date}_{end_date}",
                'category_distribution': {},
                'processed_count': 0,
                'skipped_count': 0
            }
            
            # ëˆ„ë½ ë°ì´í„° ì „ìš© ë°°ì¹˜ ì²˜ë¦¬
            processed_count, skipped_count = await self._process_missing_data_parallel(
                batch_rows, 
                start_index, 
                stats, 
                f"missing_{start_date}_{end_date}"
            )
            
            end_time = time.time()
            duration = self._format_duration(end_time - start_time)
            
            result = {
                "status": "SUCCESS",
                "period": f"{start_date} ~ {end_date}",
                "total_missing_questions": actual_count,
                "processed_count": processed_count,
                "skipped_count": skipped_count,
                "duration": duration,
                "missing_by_date": missing_by_date,
                "limit_applied": limit,
                "limit_reached": bool(limit and actual_count == limit)
            }
            
            limit_info = f" (ì œí•œ: {limit}ê°œ)" if limit else ""
            log_info(f"ğŸ‰ ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ{limit_info}!")
            log_info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {processed_count}ê°œ ì²˜ë¦¬, {skipped_count}ê°œ ì¤‘ë³µ ìŠ¤í‚µ")
            
            # 3. ì²˜ë¦¬ í›„ ê²€ì¦
            log_info("ğŸ” ì²˜ë¦¬ í›„ ê²€ì¦ ì¤‘...")
            verification_result = await self._verify_missing_data_processing(start_date, end_date)
            result["verification"] = verification_result
            
            return result
            
        except Exception as e:
            raise BatchProcessError(f"ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    async def _process_missing_data_parallel(self, rows: List[tuple], start_index: int, 
                                           stats: Dict[str, Any], target_identifier: str) -> Tuple[int, int]:
        """ëˆ„ë½ ë°ì´í„° ì „ìš© ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ - ì§„í–‰ë¥  í‘œì‹œ ë° ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹"""
        log_info(f"ğŸš€ ëˆ„ë½ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(rows)}ê°œ í•­ëª© (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)")
        log_info("âš ï¸ ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ëª¨ë“œ: ì¤‘ë³µ ì²´í¬ ë¹„í™œì„±í™” (ì´ë¯¸ ëˆ„ë½ í™•ì¸ëœ ë°ì´í„°)")
        
        # 1. ì¹´í…Œê³ ë¦¬ ìºì‹œ êµ¬ì¶•
        category_cache = await self._build_category_cache()
        
        # 2. ìŠ¤í‚¤ë§ˆ í™•ì¸ (ì¤‘ë³µ ì²´í¬ ìºì‹œëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ)
        schema_query = self.queries.get_table_schema("admin_chat_keywords")
        schema = await self.db_manager.execute_query(schema_query)
        available_columns = [row[0] for row in schema]
        query_column = self._determine_query_column(available_columns)
        
        # 3. ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        chunk_size = self.config.batch.batch_size
        chunks = [rows[i:i + chunk_size] for i in range(start_index, len(rows), chunk_size)]
        
        total_processed = 0
        total_skipped = 0
        total_chunks = len(chunks)
        total_items = len(rows) - start_index
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ ë½
        lock = threading.Lock()
        
        log_info(f"ğŸ“¦ ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ìƒì„¸ ì •ë³´:")
        log_info(f"   ğŸ“Š ì´ {total_items:,}ê°œ ëˆ„ë½ í•­ëª©ì„ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        log_info(f"   ğŸ“¦ ì²­í¬ë‹¹ ìµœëŒ€ {chunk_size}ê°œ í•­ëª©")
        log_info(f"   ğŸ ì‹œì‘ ì¸ë±ìŠ¤: {start_index}")
        log_info(f"   ğŸ”„ ì¤‘ë³µ ì²´í¬: ë¹„í™œì„±í™” (ëˆ„ë½ ë°ì´í„° ëª¨ë“œ)")
        
        start_time = time.time()
        
        # 4. ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ - ì¦‰ì‹œ ì ì¬ ë°©ì‹
        for chunk_idx, chunk in enumerate(chunks):
            chunk_start_time = time.time()
            
            # ğŸ“Š ì§„í–‰ë¥  ê³„ì‚°
            progress_percentage = ((chunk_idx) / total_chunks) * 100
            processed_items = chunk_idx * chunk_size
            remaining_items = total_items - processed_items
            
            log_info(f"ğŸ“ˆ ëˆ„ë½ ë°ì´í„° ì§„í–‰ë¥ : {progress_percentage:.1f}% ({chunk_idx}/{total_chunks} ì²­í¬)")
            log_info(f"   ğŸ”„ í˜„ì¬ ì²­í¬ {chunk_idx + 1}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ ëˆ„ë½ í•­ëª©)")
            log_info(f"   ğŸ“‹ ì²˜ë¦¬ ì™„ë£Œ: {processed_items:,}ê°œ / ë‚¨ì€ í•­ëª©: {remaining_items:,}ê°œ")

            try:
                # ì²­í¬ ì²˜ë¦¬ (ë™ê¸° ë°©ì‹) - ì¤‘ë³µ ì²´í¬ ì—†ì´
                chunk_processed, chunk_skipped, chunk_batch_data = self._process_missing_chunk_sync_no_duplicate_check(
                    chunk, category_cache, query_column, lock, stats
                )
                
                # ğŸ”¥ ì¦‰ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬ (ë©”ëª¨ë¦¬ì— ëˆ„ì í•˜ì§€ ì•ŠìŒ)
                if chunk_batch_data:
                    log_info(f"   ğŸ’¾ ëˆ„ë½ ë°ì´í„° ì²­í¬ {chunk_idx + 1} MySQL ì¦‰ì‹œ ì ì¬: {len(chunk_batch_data)}ê°œ ë ˆì½”ë“œ")
                    await self._process_immediate_batch_insert(chunk_batch_data, query_column, chunk_idx + 1, total_chunks)
                    log_info(f"   âœ… ëˆ„ë½ ë°ì´í„° ì²­í¬ {chunk_idx + 1} MySQL ì ì¬ ì™„ë£Œ")
                else:
                    log_info(f"   â­ï¸ ëˆ„ë½ ë°ì´í„° ì²­í¬ {chunk_idx + 1}: ì ì¬í•  ë°ì´í„° ì—†ìŒ")
                
                total_processed += chunk_processed
                total_skipped += chunk_skipped
                
                # ğŸ“Š ì²­í¬ ì²˜ë¦¬ ì‹œê°„ ë° ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
                chunk_duration = time.time() - chunk_start_time
                avg_chunk_time = (time.time() - start_time) / (chunk_idx + 1)
                remaining_chunks = total_chunks - (chunk_idx + 1)
                estimated_remaining_time = avg_chunk_time * remaining_chunks
                
                log_info(f"   âœ… ëˆ„ë½ ë°ì´í„° ì²­í¬ {chunk_idx + 1} ì™„ë£Œ: {chunk_processed}ê°œ ì²˜ë¦¬, {chunk_skipped}ê°œ ìŠ¤í‚µ")
                log_info(f"   â±ï¸ ì²­í¬ ì²˜ë¦¬ ì‹œê°„: {self._format_duration(chunk_duration)}")
                log_info(f"   ğŸ“ˆ ëˆ„ì  ì²˜ë¦¬: {total_processed:,}ê°œ ì™„ë£Œ, {total_skipped:,}ê°œ ìŠ¤í‚µ")
                
                if remaining_chunks > 0:
                    log_info(f"   ğŸ• ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {self._format_duration(estimated_remaining_time)}")
                
                # ì²­í¬ ê°„ ì ì‹œ ëŒ€ê¸° (ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(0.2)
                
            except Exception as e:
                log_error(f"âŒ ëˆ„ë½ ë°ì´í„° ì²­í¬ {chunk_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ğŸ‰ ìµœì¢… ì™„ë£Œ í†µê³„
        total_duration = time.time() - start_time
        final_progress = 100.0
        
        log_info(f"ğŸ‰ ëˆ„ë½ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì™„ë£Œ!")
        log_info(f"   ğŸ“ˆ ìµœì¢… ì§„í–‰ë¥ : {final_progress}% (ì™„ë£Œ)")
        log_info(f"   ğŸ“Š ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼:")
        log_info(f"      âœ… ì„±ê³µ: {total_processed:,}ê°œ")
        log_info(f"      â­ï¸ ìŠ¤í‚µ: {total_skipped:,}ê°œ")
        log_info(f"      ğŸ“¦ ì´ ì²­í¬: {total_chunks}ê°œ")
        log_info(f"      â±ï¸ ì´ ì†Œìš” ì‹œê°„: {self._format_duration(total_duration)}")
        if total_duration > 0:
            log_info(f"      ğŸ“ˆ í‰ê·  ì²˜ë¦¬ ì†ë„: {(total_processed + total_skipped) / total_duration:.1f}ê°œ/ì´ˆ")
        
        return total_processed, total_skipped

    def _process_missing_chunk_sync_no_duplicate_check(self, chunk_data: List[tuple], category_cache: Dict[str, int],
                                                       query_column: str, lock: threading.Lock,
                                                       stats: Dict[str, Any]) -> Tuple[int, int, List[Dict[str, Any]]]:
        """ëˆ„ë½ ë°ì´í„° ì²­í¬ ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜ - ì¤‘ë³µ ì²´í¬ ì—†ì´ ë°”ë¡œ ì²˜ë¦¬"""
        chunk_processed = 0
        chunk_skipped = 0
        chunk_batch_data = []
        total_chunk_items = len(chunk_data)
        
        # stats ì•ˆì „ì„± í™•ì¸
        if stats is None:
            stats = {}
        if 'category_distribution' not in stats:
            stats['category_distribution'] = {}
        
        log_info(f"      ğŸ”§ ì²­í¬ ë‚´ë¶€ ì²˜ë¦¬ ì‹œì‘: {total_chunk_items}ê°œ í•­ëª© (ì¤‘ë³µ ì²´í¬ ë¹„í™œì„±í™”)")
        
        for item_idx, row in enumerate(chunk_data):
            try:
                # ğŸ“Š ì²­í¬ ë‚´ ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 10% ê°„ê²© ë˜ëŠ” í° ì²­í¬ì˜ ê²½ìš°)
                if total_chunk_items > 20 and (item_idx + 1) % max(1, total_chunk_items // 10) == 0:
                    chunk_progress = ((item_idx + 1) / total_chunk_items) * 100
                    log_info(f"         ğŸ“ˆ ì²­í¬ ë‚´ ì§„í–‰ë¥ : {chunk_progress:.0f}% ({item_idx + 1}/{total_chunk_items})")
                elif total_chunk_items <= 20 and (item_idx + 1) % 5 == 0:
                    log_info(f"         ğŸ“‹ ì²˜ë¦¬ ì¤‘: {item_idx + 1}/{total_chunk_items} í•­ëª©")
                
                input_text = row[0]
                total_count = row[1]
                created_at = row[2]
                
                # HCX ë¶„ë¥˜ ì²˜ë¦¬ (ì¤‘ë³µ ì²´í¬ ì—†ì´ ë°”ë¡œ ì²˜ë¦¬)
                log_info(f"         ğŸ¤– HCX API í˜¸ì¶œ: {input_text[:50]}...")
                keyword_categories = self.hcx_service.classify_education_question(input_text)
                
                # ğŸš¦ API í˜¸ì¶œ ê°„ ì§§ì€ ì§€ì—° ì¶”ê°€ (Rate Limiting ë°©ì§€)
                time.sleep(0.1)  # 100ms ì§€ì—°
                
                # í‚¤ì›Œë“œ-ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
                question_processed = False  # ì´ ì§ˆë¬¸ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì²´í¬
                
                for item in keyword_categories:
                    keyword = item.get("keyword", "").strip()
                    categories = item.get("categories", ["ê¸°íƒ€"])
                    
                    if not keyword:
                        continue
                    
                    # ğŸ”§ í‚¤ì›Œë“œ ê¸¸ì´ ì œí•œ (ë°ì´í„°ë² ì´ìŠ¤ ì»¬ëŸ¼ í¬ê¸° ê³ ë ¤)
                    if len(keyword) > 100:  # VARCHAR(100) ê°€ì •
                        log_warning(f"         âš ï¸ í‚¤ì›Œë“œ ê¸¸ì´ ì´ˆê³¼ ({len(keyword)}ì), ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´: {keyword[:50]}...")
                        keyword = input_text[:95] + "..." if len(input_text) > 95 else input_text
                        if len(keyword) > 100:
                            keyword = keyword[:97] + "..."
                    
                    # ë¹ˆ í‚¤ì›Œë“œë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ ì²´í¬
                    if not keyword or keyword.strip() == "" or keyword.strip() == "ê¸°íƒ€":
                        meaningful_keyword = self._extract_simple_keyword(input_text)
                        keyword = meaningful_keyword if meaningful_keyword else "ê¸°íƒ€"
                        log_info(f"         ğŸ”„ í‚¤ì›Œë“œ ë³´ì™„: '{keyword}'")
                    
                    for category_name in categories:
                        category_id = category_cache.get(str(category_name).strip())
                        if not category_id:
                            continue
                        
                        # ì¤‘ë³µ ì²´í¬ ì—†ì´ ë°”ë¡œ ë°ì´í„° ì¶”ê°€
                        chunk_batch_data.append({
                            query_column: input_text,
                            "keyword": keyword,
                            "category_id": category_id,
                            "query_count": total_count,
                            "created_at": created_at,
                            "batch_created_at": self.batch_created_at
                        })
                        
                        chunk_processed += 1
                        question_processed = True
                        log_info(f"         âœ… ë°ì´í„° ì¶”ê°€ (ì¤‘ë³µì²´í¬ ì—†ì´): {keyword} - {category_name}")
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸ (ì•ˆì „í•˜ê²Œ)
                        try:
                            with lock:
                                if 'category_distribution' in stats:
                                    stats['category_distribution'][str(category_name)] = \
                                        stats['category_distribution'].get(str(category_name), 0) + 1
                        except Exception as stats_error:
                            # í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                            pass
                
                # ì§ˆë¬¸ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µìœ¼ë¡œ ì¹´ìš´íŠ¸
                if not question_processed:
                    chunk_skipped += 1
                    log_info(f"         â­ï¸ ì§ˆë¬¸ ìŠ¤í‚µ (ë¶„ë¥˜ ê²°ê³¼ ì—†ìŒ): {input_text[:30]}...")
                
            except Exception as e:
                chunk_skipped += 1
                log_error(f"         âŒ ì²­í¬ ë‚´ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {input_text} | ì˜¤ë¥˜: {e}")
                continue
        
        log_info(f"      âœ… ì²­í¬ ë‚´ë¶€ ì²˜ë¦¬ ì™„ë£Œ: {chunk_processed}ê°œ ì²˜ë¦¬, {chunk_skipped}ê°œ ìŠ¤í‚µ (ì¤‘ë³µì²´í¬ ë¹„í™œì„±í™”)")
        return chunk_processed, chunk_skipped, chunk_batch_data

    async def _fetch_data_for_period(self, start_date: str, end_date: str) -> List[tuple]:
        """ê¸°ê°„ë³„ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            query = self.queries.get_unique_chattings_by_date(start_date, end_date)
            return await self.db_manager.execute_query(query)
        except Exception as e:
            raise BatchProcessError(f"ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")

    async def _build_category_cache(self) -> Dict[str, int]:
        """ì¹´í…Œê³ ë¦¬ ìºì‹œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        log_info("ğŸ” ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¯¸ë¦¬ ì¡°íšŒ ì¤‘...")
        try:
            query = self.queries.get_categories()
            categories = await self.db_manager.execute_query(query)
            cache = {cat[1]: cat[0] for cat in categories}
            log_info(f"âœ… {len(cache)}ê°œ ì¹´í…Œê³ ë¦¬ ìºì‹œ ì™„ë£Œ")
            return cache
        except Exception as e:
            raise BatchProcessError(f"ì¹´í…Œê³ ë¦¬ ìºì‹œ êµ¬ì¶• ì‹¤íŒ¨: {e}")
    
    async def _verify_missing_data_processing(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ í›„ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        log_info("ğŸ” ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ê²€ì¦ ì¤‘...")
        
        try:
            # ì²˜ë¦¬ í›„ ë‹¤ì‹œ ëˆ„ë½ ë°ì´í„° í™•ì¸
            verification_query = self.queries.verify_missing_data_processing(
                start_date, end_date
            )
            
            remaining_result = await self.db_manager.execute_query(verification_query)
            
            total_remaining = sum(row[1] for row in remaining_result)
            
            verification = {
                "remaining_missing_count": total_remaining,
                "remaining_by_date": {str(row[0]): row[1] for row in remaining_result},
                "verification_success": total_remaining == 0
            }
            
            if total_remaining == 0:
                log_info("âœ… ê²€ì¦ ì™„ë£Œ: ëª¨ë“  ëˆ„ë½ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                log_warning(f"âš ï¸ ê²€ì¦ ê²°ê³¼: {total_remaining}ê°œì˜ ë°ì´í„°ê°€ ì—¬ì „íˆ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                for date, count in verification["remaining_by_date"].items():
                    log_info(f"   - {date}: {count}ê°œ")
            
            return verification
            
        except Exception as e:
            log_error(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"verification_success": False, "error": str(e)}

    async def _process_large_batch_insert(self, data_list: List[Dict[str, Any]], query_column: str):
        """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ INSERT ì²˜ë¦¬"""
        if not data_list:
            return
        
        log_info(f"ğŸ’¾ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ INSERT ì‹œì‘: {len(data_list)}ê°œ ë ˆì½”ë“œ")
        
        # ë°ì´í„°ë¥¼ ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• 
        batches = [data_list[i:i + self.config.batch.batch_size] 
                   for i in range(0, len(data_list), self.config.batch.batch_size)]
        
        insert_query = self.queries.insert_chat_keywords(query_column)
        
        success_count = 0
        
        for batch_idx, batch in enumerate(batches):
            try:
                log_info(f"ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
                
                batch_success = await self.db_manager.execute_batch_insert(insert_query, batch)
                success_count += batch_success
                
                # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(0.1)
                
            except Exception as e:
                log_error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì‹¤íŒ¨: {e}")
        
        log_info(f"ğŸ¯ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ INSERT ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")

    async def _process_immediate_batch_insert(self, data_list: List[Dict[str, Any]], query_column: str, 
                                            current_chunk: int, total_chunks: int):
        """ì¦‰ì‹œ ë°°ì¹˜ INSERT ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """
        if not data_list:
            log_info(f"      ğŸ’¾ ì²­í¬ {current_chunk}/{total_chunks}: ì ì¬í•  ë°ì´í„° ì—†ìŒ")
            return
        
        log_info(f"      ğŸ’¾ ì²­í¬ {current_chunk}/{total_chunks} MySQL ì¦‰ì‹œ INSERT ì‹œì‘:")
        log_info(f"         ğŸ“Š ì ì¬ ëŒ€ìƒ: {len(data_list)}ê°œ ë ˆì½”ë“œ")
        log_info(f"         ğŸ—„ï¸ ëŒ€ìƒ í…Œì´ë¸”: admin_chat_keywords")
        log_info(f"         ğŸ“ ì»¬ëŸ¼: {query_column}")
        
        insert_query = self.queries.insert_chat_keywords(query_column)
        
        try:
            insert_start_time = time.time()
            
            # í•œ ë²ˆì— ëª¨ë“  ë°ì´í„° INSERT (ì²­í¬ í¬ê¸°ê°€ ì´ë¯¸ ì ë‹¹í•¨)
            log_info(f"         ğŸš€ ë°°ì¹˜ INSERT ì‹¤í–‰ ì¤‘...")
            success_count = await self.db_manager.execute_batch_insert(insert_query, data_list)
            
            insert_duration = time.time() - insert_start_time
            
            log_info(f"         âœ… ë°°ì¹˜ INSERT ì„±ê³µ: {success_count}ê°œ ë ˆì½”ë“œ")
            log_info(f"         â±ï¸ INSERT ì†Œìš” ì‹œê°„: {self._format_duration(insert_duration)}")
            log_info(f"         ğŸ“ˆ INSERT ì†ë„: {success_count / insert_duration:.1f}ê°œ/ì´ˆ")
            
        except Exception as e:
            log_error(f"         âŒ ë°°ì¹˜ INSERT ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")
            log_info(f"         ğŸ”„ ê°œë³„ INSERT ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            # ì‹¤íŒ¨ ì‹œ ê°œë³„ INSERTë¡œ ì¬ì‹œë„
            await self._fallback_individual_insert(data_list, insert_query, query_column)

    async def _fallback_individual_insert(self, data_list: List[Dict[str, Any]], insert_query: str, query_column: str):
        """ë°°ì¹˜ INSERT ì‹¤íŒ¨ ì‹œ ê°œë³„ INSERTë¡œ ì²˜ë¦¬"""
        log_info(f"         ğŸ”„ ê°œë³„ INSERT ì‹œì‘: {len(data_list)}ê°œ ë ˆì½”ë“œ")
        
        success_count = 0
        failed_count = 0
        individual_start_time = time.time()
        
        for idx, params in enumerate(data_list):
            try:
                # í‚¤ì›Œë“œ ê¸¸ì´ ìµœì¢… ì•ˆì „ì¥ì¹˜
                if 'keyword' in params:
                    keyword = params['keyword']
                    if len(str(keyword)) > 100:
                        log_warning(f"            âš ï¸ ê°œë³„ INSERT í‚¤ì›Œë“œ ê¸¸ì´ ì´ˆê³¼, ìë¥´ê¸°: {len(str(keyword))}ì -> 100ì")
                        params['keyword'] = str(keyword)[:98] + "..."
                
                await self.db_manager.execute_insert(insert_query, params)
                success_count += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 10ê°œë§ˆë‹¤)
                if (idx + 1) % 10 == 0 or idx == len(data_list) - 1:
                    progress = ((idx + 1) / len(data_list)) * 100
                    log_info(f"            ğŸ“Š ê°œë³„ INSERT ì§„í–‰ë¥ : {progress:.0f}% ({idx + 1}/{len(data_list)}) - ì„±ê³µ: {success_count}ê°œ")
                
            except Exception as e:
                failed_count += 1
                log_error(f"            âŒ ê°œë³„ INSERT ì‹¤íŒ¨ ({idx + 1}): {e}")
                # í‚¤ì›Œë“œë§Œ ë¡œê¹… (ì „ì²´ ë°ì´í„°ëŠ” ë„ˆë¬´ ê¸¸ì–´ì§)
                keyword = params.get('keyword', 'Unknown')[:50]
                log_info(f"               ì‹¤íŒ¨í•œ í‚¤ì›Œë“œ: {keyword}")
        
        individual_duration = time.time() - individual_start_time
        
        log_info(f"         ğŸ¯ ê°œë³„ INSERT ì™„ë£Œ:")
        log_info(f"            âœ… ì„±ê³µ: {success_count}ê°œ")
        log_info(f"            âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        log_info(f"            â±ï¸ ì†Œìš” ì‹œê°„: {self._format_duration(individual_duration)}")
        if individual_duration > 0:
            log_info(f"            ğŸ“ˆ í‰ê·  ì†ë„: {(success_count + failed_count) / individual_duration:.1f}ê°œ/ì´ˆ")

    def _determine_query_column(self, available_columns: List[str]) -> str:
        """ì ì ˆí•œ ì¿¼ë¦¬ ì»¬ëŸ¼ëª…ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        column_priority = ['query_text', 'content', 'question', 'text']
        
        for col in column_priority:
            if col in available_columns:
                return col
        
        # ê¸°ë³¸ê°’
        return available_columns[1] if len(available_columns) > 1 else available_columns[0]

    def _format_duration(self, duration: float) -> str:
        """ì´ˆë¥¼ ë¶„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        minutes = int(duration // 60)
        seconds = int(duration % 60)
        return f"{minutes}ë¶„ {seconds}ì´ˆ"

    def _extract_simple_keyword(self, text: str) -> str:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
        # êµìœ¡ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_mapping = {
            "ìˆ˜ê°•ì‹ ì²­": "ìˆ˜ê°•ì‹ ì²­",
            "ì „í•™": "ì „í•™",
            "í¸ì…": "í¸ì…",
            "ì„±ì ": "ì„±ì ",
            "í‰ê°€": "í‰ê°€",
            "ì‹œí—˜": "ì‹œí—˜",
            "ì…í•™": "ì…í•™",
            "ì¡¸ì—…": "ì¡¸ì—…",
            "íœ´í•™": "íœ´í•™",
            "ë³µí•™": "ë³µí•™",
            "ì¥í•™ê¸ˆ": "ì¥í•™ê¸ˆ",
            "í•™êµí­ë ¥": "í•™êµí­ë ¥",
            "êµê¶Œ": "êµê¶Œ",
            "ê²€ì •ê³ ì‹œ": "ê²€ì •ê³ ì‹œ",
            "ì„ìš©": "ì„ìš©",
            "ì¦ëª…ì„œ": "ì¦ëª…ì„œ",
            "ìˆ˜ì—…": "ìˆ˜ì—…",
            "ê°•ì˜": "ê°•ì˜",
            "ê³¼ì œ": "ê³¼ì œ"
        }
        
        text_lower = text.lower()
        
        # 1. ë§¤í•‘ëœ í‚¤ì›Œë“œ ì°¾ê¸°
        for keyword in keyword_mapping.keys():
            if keyword in text_lower:
                return keyword
        
        # 2. ì˜ë¯¸ìˆëŠ” ì²« ë²ˆì§¸ ë‹¨ì–´ ì¶”ì¶œ
        words = text.split()
        for word in words:
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì‚¬/ì–´ë¯¸ ì œì™¸
            if len(word) >= 2 and word not in ['ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ì™€', 'ê³¼', 'í•˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””']:
                return word[:20]  # ìµœëŒ€ 20ì
        
        # 3. ê¸°ë³¸ê°’
        return text[:10].strip() if len(text) > 10 else text.strip()

    async def run_missing_data_batch(self, start_date: str, end_date: str, start_index: int = 0) -> Dict[str, Any]:
        """ëˆ„ë½ ë°ì´í„° í™•ì¸ ë° ì²˜ë¦¬ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤."""
        log_info(f"ğŸ” ëˆ„ë½ ë°ì´í„° í†µí•© ì²˜ë¦¬ ì‹œì‘: {start_date} ~ {end_date}")
        
        try:
            # 1. ëˆ„ë½ ë°ì´í„° í™•ì¸
            missing_check = await self.check_missing_data(start_date, end_date)
            
            # 2. ëˆ„ë½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
            if missing_check["stats"]["total_missing_questions"] > 0:
                process_result = await self.process_missing_data(start_date, end_date, start_index)
                
                return {
                    **missing_check,
                    "processing_result": process_result,
                    "final_status": process_result["status"]
                }
            else:
                log_info("âœ… ëˆ„ë½ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    **missing_check,
                    "processing_result": {
                        "status": "SUCCESS",
                        "message": "ì²˜ë¦¬í•  ëˆ„ë½ ë°ì´í„° ì—†ìŒ"
                    },
                    "final_status": "SUCCESS"
                }
                
        except Exception as e:
            raise BatchProcessError(f"ëˆ„ë½ ë°ì´í„° í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def run_single_batch(self, target_date: str = None, start_index: int = 0) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë‚ ì§œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            target_date (str): ëŒ€ìƒ ë‚ ì§œ
            start_index (int): ì‹œì‘ ì¸ë±ìŠ¤
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        if not target_date:
            target_date = self.config.batch.default_target_date
        
        log_info(f"ğŸ“‹ ë‹¨ì¼ ë‚ ì§œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {target_date}")
        
        result = {
            "status": "SUCCESS",
            "target_date": target_date,
            "total_rows": 0,
            "processed_count": 0,
            "skipped_count": 0,
            "duration": None
        }
        
        try:
            start_time = time.time()
            
            # ë°ì´í„° ì¡°íšŒ
            rows = await self._fetch_data_for_period(target_date, target_date)
            
            if not rows:
                log_info(f"â„¹ï¸ {target_date}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                result["total_rows"] = 0
                result["duration"] = self._format_duration(time.time() - start_time)
                return result
            
            result["total_rows"] = len(rows)
            log_info(f"âœ… {len(rows)}ê°œ ë ˆì½”ë“œ ì¡°íšŒ")
            
            # stats ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
            stats = {
                'target_date': target_date,
                'category_distribution': {},
                'processed_count': 0,
                'skipped_count': 0
            }
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            processed_count, skipped_count = await self._process_batch_data_parallel(
                rows, 
                start_index, 
                stats, 
                target_date,
                target_date,  # start_date
                target_date   # end_date
            )
            
            end_time = time.time()
            duration = self._format_duration(end_time - start_time)
            
            result.update({
                "processed_count": processed_count,
                "skipped_count": skipped_count,
                "duration": duration
            })
            
            log_info(f"âœ… {target_date} ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬, {skipped_count}ê°œ ìŠ¤í‚µ")
            return result
            
        except Exception as e:
            result["status"] = "FAILED"
            result["error"] = str(e)
            raise BatchProcessError(f"ë‹¨ì¼ ë‚ ì§œ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({target_date}): {e}")
    
    async def _process_batch_data_parallel(self, rows: List[tuple], start_index: int, 
                                         stats: Dict[str, Any], target_identifier: str, start_date: str, end_date: str) -> Tuple[int, int]:
        """ë³‘ë ¬ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ - ì§„í–‰ë¥  í‘œì‹œ ë° ì¤‘ê°„ ì ì¬"""
        # ë³€ìˆ˜ ì´ˆê¸°í™” ìˆ˜ì •
        total_processed = 0
        total_skipped = 0
        
        # statsê°€ Noneì´ê±°ë‚˜ category_distributionì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if stats is None:
            stats = {}
        if 'category_distribution' not in stats:
            stats['category_distribution'] = {}
        
        # ì¹´í…Œê³ ë¦¬ ìºì‹œ ìƒì„±
        category_cache = await self._build_category_cache()
        
        # ì¤‘ë³µ ì²´í¬ ìºì‹œ ìƒì„± - target_identifierë¥¼ ë‚ ì§œë¡œ ì‚¬ìš©
        # target_identifierê°€ ë‚ ì§œ í˜•ì‹ì¸ ê²½ìš° í•´ë‹¹ ë‚ ì§œë¥¼ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ë¡œ ì‚¬ìš©
        existing_cache, query_column = await self._build_duplicate_cache(start_date, end_date)
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ ë½
        lock = threading.Lock()
        
        # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        processing_rows = rows[start_index:]
        chunk_size = self.config.batch.classification_batch_size
        chunks = [processing_rows[i:i + chunk_size] 
                  for i in range(0, len(processing_rows), chunk_size)]
        
        total_chunks = len(chunks)
        total_items = len(processing_rows)
        
        log_info(f"ğŸš€ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘:")
        log_info(f"   ğŸ“Š ì´ {total_items:,}ê°œ í•­ëª©ì„ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        log_info(f"   ğŸ“¦ ì²­í¬ë‹¹ ìµœëŒ€ {chunk_size}ê°œ í•­ëª©")
        
        start_time = time.time()
        
        # ğŸ”¥ ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ - ì¦‰ì‹œ ì ì¬ ë°©ì‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        for chunk_idx, chunk in enumerate(chunks):
            chunk_start_time = time.time()
            
            # ğŸ“Š ì§„í–‰ë¥  ê³„ì‚°
            progress_percentage = ((chunk_idx) / total_chunks) * 100
            processed_items = chunk_idx * chunk_size
            remaining_items = total_items - processed_items
            
            log_info(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress_percentage:.1f}% ({chunk_idx}/{total_chunks} ì²­í¬)")
            log_info(f"   ğŸ”„ í˜„ì¬ ì²­í¬ {chunk_idx + 1}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ í•­ëª©)")
            log_info(f"   ğŸ“‹ ì²˜ë¦¬ ì™„ë£Œ: {processed_items:,}ê°œ / ë‚¨ì€ í•­ëª©: {remaining_items:,}ê°œ")
            
            try:
                # ì²­í¬ ì²˜ë¦¬ (ë™ê¸° ë°©ì‹)
                chunk_processed, chunk_skipped, chunk_batch_data = self._process_chunk_sync(
                    chunk, category_cache, existing_cache, query_column, lock, stats
                )
                
                # ğŸ”¥ ì¦‰ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬ (ë©”ëª¨ë¦¬ì— ëˆ„ì í•˜ì§€ ì•ŠìŒ)
                if chunk_batch_data:
                    log_info(f"   ğŸ’¾ ì²­í¬ {chunk_idx + 1} MySQL ì¦‰ì‹œ ì ì¬ ì‹œì‘: {len(chunk_batch_data)}ê°œ ë ˆì½”ë“œ")
                    await self._process_immediate_batch_insert(chunk_batch_data, query_column, chunk_idx + 1, total_chunks)
                    log_info(f"   âœ… ì²­í¬ {chunk_idx + 1} MySQL ì ì¬ ì™„ë£Œ")
                else:
                    log_info(f"   â­ï¸ ì²­í¬ {chunk_idx + 1}: ì ì¬í•  ë°ì´í„° ì—†ìŒ")
                
                total_processed += chunk_processed
                total_skipped += chunk_skipped
                
                # ğŸ“Š ì²­í¬ ì²˜ë¦¬ ì‹œê°„ ë° ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
                chunk_duration = time.time() - chunk_start_time
                avg_chunk_time = (time.time() - start_time) / (chunk_idx + 1)
                remaining_chunks = total_chunks - (chunk_idx + 1)
                estimated_remaining_time = avg_chunk_time * remaining_chunks
                
                log_info(f"   âœ… ì²­í¬ {chunk_idx + 1} ì™„ë£Œ: {chunk_processed}ê°œ ì²˜ë¦¬, {chunk_skipped}ê°œ ìŠ¤í‚µ")
                log_info(f"   â±ï¸ ì²­í¬ ì²˜ë¦¬ ì‹œê°„: {self._format_duration(chunk_duration)}")
                log_info(f"   ğŸ“ˆ ëˆ„ì  ì²˜ë¦¬: {total_processed:,}ê°œ ì™„ë£Œ, {total_skipped:,}ê°œ ìŠ¤í‚µ")
                
                if remaining_chunks > 0:
                    log_info(f"   ğŸ• ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {self._format_duration(estimated_remaining_time)}")
                
                # ì²­í¬ ê°„ ì ì‹œ ëŒ€ê¸° (ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(0.3)
                
            except Exception as e:
                log_error(f"âŒ ì²­í¬ {chunk_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ğŸ‰ ìµœì¢… ì™„ë£Œ í†µê³„
        total_duration = time.time() - start_time
        final_progress = 100.0
        
        log_info(f"ğŸ‰ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ!")
        log_info(f"   ğŸ“ˆ ìµœì¢… ì§„í–‰ë¥ : {final_progress}% (ì™„ë£Œ)")
        log_info(f"   ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        log_info(f"      âœ… ì„±ê³µ: {total_processed:,}ê°œ")
        log_info(f"      â­ï¸ ìŠ¤í‚µ: {total_skipped:,}ê°œ")
        log_info(f"      ğŸ“¦ ì´ ì²­í¬: {total_chunks}ê°œ")
        log_info(f"      â±ï¸ ì´ ì†Œìš” ì‹œê°„: {self._format_duration(total_duration)}")
        if total_duration > 0:
            log_info(f"      ğŸ“ˆ í‰ê·  ì²˜ë¦¬ ì†ë„: {(total_processed + total_skipped) / total_duration:.1f}ê°œ/ì´ˆ")
        
        return total_processed, total_skipped
    
    def _process_chunk_sync(self, chunk_data: List[tuple], category_cache: Dict[str, int],
                           existing_cache: set, query_column: str, lock: threading.Lock,
                           stats: Dict[str, Any]) -> Tuple[int, int, List[Dict[str, Any]]]:
        """ë™ê¸° ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜ - ìƒì„¸ ì§„í–‰ë¥  í‘œì‹œ"""
        chunk_processed = 0
        chunk_skipped = 0
        chunk_batch_data = []
        total_chunk_items = len(chunk_data)
        
        # stats ì•ˆì „ì„± í™•ì¸
        if stats is None:
            stats = {}
        if 'category_distribution' not in stats:
            stats['category_distribution'] = {}
        
        log_info(f"      ğŸ”§ ì²­í¬ ë‚´ë¶€ ì²˜ë¦¬ ì‹œì‘: {total_chunk_items}ê°œ í•­ëª©")
        
        for item_idx, row in enumerate(chunk_data):
            try:
                # ğŸ“Š ì²­í¬ ë‚´ ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 10% ê°„ê²© ë˜ëŠ” í° ì²­í¬ì˜ ê²½ìš°)
                if total_chunk_items > 20 and (item_idx + 1) % max(1, total_chunk_items // 10) == 0:
                    chunk_progress = ((item_idx + 1) / total_chunk_items) * 100
                    log_info(f"         ğŸ“ˆ ì²­í¬ ë‚´ ì§„í–‰ë¥ : {chunk_progress:.0f}% ({item_idx + 1}/{total_chunk_items})")
                elif total_chunk_items <= 20 and (item_idx + 1) % 5 == 0:
                    log_info(f"         ğŸ“‹ ì²˜ë¦¬ ì¤‘: {item_idx + 1}/{total_chunk_items} í•­ëª©")
                
                input_text = row[0]
                total_count = row[1]
                created_at = row[2]
                
                # HCX ë¶„ë¥˜ ì²˜ë¦¬
                log_info(f"         ğŸ¤– HCX API í˜¸ì¶œ: {input_text[:50]}...")
                keyword_categories = self.hcx_service.classify_education_question(input_text)
                
                # ğŸš¦ API í˜¸ì¶œ ê°„ ì§§ì€ ì§€ì—° ì¶”ê°€ (Rate Limiting ë°©ì§€)
                time.sleep(0.1)  # 100ms ì§€ì—°
                
                # í‚¤ì›Œë“œ-ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
                for item in keyword_categories:
                    keyword = item.get("keyword", "").strip()
                    categories = item.get("categories", ["ê¸°íƒ€"])
                    
                    if not keyword:
                        continue
                    
                    # ğŸ”§ í‚¤ì›Œë“œ ê¸¸ì´ ì œí•œ (ë°ì´í„°ë² ì´ìŠ¤ ì»¬ëŸ¼ í¬ê¸° ê³ ë ¤)
                    # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ì¤„ì…ë‹ˆë‹¤
                    if len(keyword) > 100:  # VARCHAR(100) ê°€ì • - ì‚¬ìš©ì ë³€ê²½ ë°˜ì˜
                        log_warning(f"         âš ï¸ í‚¤ì›Œë“œ ê¸¸ì´ ì´ˆê³¼ ({len(keyword)}ì), ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´: {keyword[:50]}...")
                        # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ê¸¸ë©´ ì›ë³¸ ì§ˆë¬¸ì˜ ì•ë¶€ë¶„ìœ¼ë¡œ ëŒ€ì²´
                        keyword = input_text[:95] + "..." if len(input_text) > 95 else input_text
                        if len(keyword) > 100:
                            keyword = keyword[:97] + "..."
                    
                    # ë¹ˆ í‚¤ì›Œë“œë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ ì²´í¬
                    if not keyword or keyword.strip() == "" or keyword.strip() == "ê¸°íƒ€":
                        # ì›ë³¸ ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
                        meaningful_keyword = self._extract_simple_keyword(input_text)
                        keyword = meaningful_keyword if meaningful_keyword else "ê¸°íƒ€"
                        log_info(f"         ğŸ”„ í‚¤ì›Œë“œ ë³´ì™„: '{keyword}'")
                    
                    for category_name in categories:
                        category_id = category_cache.get(str(category_name).strip())
                        if not category_id:
                            continue
                        
                        chunk_batch_data.append({
                            query_column: input_text,
                            "keyword": keyword,
                            "category_id": category_id,
                            "query_count": total_count,
                            "created_at": created_at,
                            "batch_created_at": self.batch_created_at
                        })
                        
                        chunk_processed += 1
                        log_info(f"         âœ… ë°ì´í„° ì¶”ê°€: {keyword} - {category_name}")
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸ (ì•ˆì „í•˜ê²Œ)
                        try:
                            with lock:
                                if 'category_distribution' in stats:
                                    stats['category_distribution'][str(category_name)] = \
                                        stats['category_distribution'].get(str(category_name), 0) + 1
                        except Exception as stats_error:
                            # í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (í•µì‹¬ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
                            pass
                
            except Exception as e:
                log_error(f"         âŒ ì²­í¬ ë‚´ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {input_text} | ì˜¤ë¥˜: {e}")
                continue
        
        log_info(f"      âœ… ì²­í¬ ë‚´ë¶€ ì²˜ë¦¬ ì™„ë£Œ: {chunk_processed}ê°œ ì²˜ë¦¬, {chunk_skipped}ê°œ ìŠ¤í‚µ")
        return chunk_processed, chunk_skipped, chunk_batch_data

    async def _build_duplicate_cache(self, start_date: str = None, end_date: str = None) -> Tuple[set, str]:
        """ì¤‘ë³µ ì²´í¬ ìºì‹œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        log_info("ğŸ” ê¸°ì¡´ ë°ì´í„° ì¤‘ë³µ ì²´í¬ìš© ìºì‹œ ìƒì„± ì¤‘...")
        
        try:
            schema_query = self.queries.get_table_schema("admin_chat_keywords")
            schema = await self.db_manager.execute_query(schema_query)
            available_columns = [row[0] for row in schema]
            
            # ì»¬ëŸ¼ëª… ë§¤í•‘
            query_column = self._determine_query_column(available_columns)
            
            # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ - ë‚ ì§œ ë²”ìœ„ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¡°íšŒ
            if start_date and end_date:
                existing_data_query = self.queries.get_existing_keywords_cache(start_date, end_date)
                existing_result = await self.db_manager.execute_query(existing_data_query)
                existing_cache = {row[0] for row in existing_result}
                log_info(f"âœ… ê¸°ì¡´ ë°ì´í„° {len(existing_cache)}ê°œ ìºì‹œ ì™„ë£Œ ({start_date} ~ {end_date})")
            else:
                # ë‚ ì§œ ë²”ìœ„ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ìºì‹œ ì‚¬ìš©
                existing_cache = set()
                log_info("â„¹ï¸ ë‚ ì§œ ë²”ìœ„ ì—†ìŒ, ë¹ˆ ìºì‹œë¡œ ì‹œì‘")
            
            return existing_cache, query_column
            
        except Exception as e:
            log_warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ìºì‹œ ìƒì„± ì‹¤íŒ¨, ë¹ˆ ìºì‹œë¡œ ì‹œì‘: {e}")
            return set(), "query_text"

    def _normalize_date_for_cache(self, created_at):
        """
        ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ë‚ ì§œ ì •ê·œí™” í•¨ìˆ˜
        ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
        
        Args:
            created_at: datetime, date, str ì¤‘ í•˜ë‚˜ì˜ í˜•ì‹
            
        Returns:
            str: 'YYYY-MM-DD' í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´
        """
        try:
            from datetime import datetime, date
            
            # 1. datetime ê°ì²´ì¸ ê²½ìš°
            if isinstance(created_at, datetime):
                return created_at.strftime('%Y-%m-%d')
            
            # 2. date ê°ì²´ì¸ ê²½ìš°
            if isinstance(created_at, date):
                return created_at.strftime('%Y-%m-%d')
            
            # 3. ë¬¸ìì—´ì¸ ê²½ìš°
            if isinstance(created_at, str):
                # ì´ë¯¸ YYYY-MM-DD í˜•ì‹ì¸ì§€ í™•ì¸
                if len(created_at) >= 10 and created_at[:10].count('-') == 2:
                    return created_at[:10]
                
                # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                try:
                    # 'YYYY-MM-DD HH:MM:SS' í˜•ì‹ ì‹œë„
                    dt = datetime.strptime(created_at, '%Y-%m-%d %H:%M:%S')
                    return dt.strftime('%Y-%m-%d')
                except ValueError:
                    pass
                
                try:
                    # 'YYYY-MM-DD' í˜•ì‹ ì‹œë„
                    dt = datetime.strptime(created_at, '%Y-%m-%d')
                    return dt.strftime('%Y-%m-%d')
                except ValueError:
                    pass
            
            # 4. hasattrë¥¼ ì´ìš©í•œ ê¸°ì¡´ ë°©ì‹ (fallback)
            if hasattr(created_at, 'strftime'):
                return created_at.strftime('%Y-%m-%d')
            
            # 5. ìµœí›„ì˜ ìˆ˜ë‹¨: ë¬¸ìì—´ ë³€í™˜ í›„ ì• 10ìë¦¬
            return str(created_at)[:10]
            
        except Exception as e:
            log_warning(f"âš ï¸ ë‚ ì§œ ì •ê·œí™” ì‹¤íŒ¨: {created_at} -> {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’
            return str(created_at)[:10] if created_at else "1970-01-01" 